{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##先写数据增强的过程\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from albumentations import Compose as Compose_albu\n",
    "from albumentations import LongestMaxSize, PadIfNeeded\n",
    "\n",
    "\n",
    "def to_numpy(data):\n",
    "    image, label = data['image'], data['label']\n",
    "    data['image'] = np.array(image)\n",
    "    if data['label'] is not None:\n",
    "        data['label'] = np.array(label)\n",
    "    return data\n",
    "\n",
    "\n",
    "class MedicalTransform:\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        if isinstance(output_size, (tuple, list)):\n",
    "            self._output_size = output_size\n",
    "        else:\n",
    "            self._output_size = (output_size, output_size)\n",
    "\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        data = to_numpy(data)\n",
    "        img, label = data['image'], data['label']\n",
    "        is_3d = True if img.shape == 4 else False\n",
    "        max_size = max(self._output_size[0], self._output_size[1])\n",
    "        task = [\n",
    "            LongestMaxSize(max_size, p=1),\n",
    "            PadIfNeeded(self._output_size[0], self._output_size[1])\n",
    "        ]\n",
    "\n",
    "        aug = Compose_albu(task)\n",
    "        if not is_3d:\n",
    "            aug_data = aug(image=img, mask=label)\n",
    "            data['image'], data['label'] = aug_data['image'], aug_data['mask']\n",
    "\n",
    "        else:\n",
    "            keys = {}\n",
    "            targets = {}\n",
    "            for i in range(1, img.shape[2]):\n",
    "                keys.update({f'image{i}': 'image'})\n",
    "                keys.update({f'mask{i}': 'mask'})\n",
    "                targets.update({f'image{i}': img[:, :, i]})\n",
    "                targets.update({f'mask{i}': label[:, :, i]})\n",
    "            aug.add_targets(keys)\n",
    "            \n",
    "            targets.update({'image': img[:, :, 0]})\n",
    "            targets.update({'mask': label[:, :, 0]})\n",
    "            \n",
    "            aug_data = aug(**targets)\n",
    "            imgs = [aug_data['image']]\n",
    "            labels = [aug_data['mask']]\n",
    "            \n",
    "            for i in range(1, img.shape[2]):\n",
    "                imgs.append(aug_data[f'image{i}'])\n",
    "                labels.append(aug_data[f'mask{i}'])\n",
    "            \n",
    "            img = np.stack(imgs, axis=-1)\n",
    "            label = np.stack(labels, axis=-1)\n",
    "            data['image'] = img\n",
    "            data['label'] = label\n",
    "        \n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##网络定义\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "class DenseUNet(nn.Module):\n",
    "    def __init__(self, in_ch=3, out_ch=3):\n",
    "        super(DenseUNet, self).__init__()\n",
    "        densenet = models.densenet161(pretrained=True)\n",
    "        backbone = list(list(densenet.children())[0].children())\n",
    "        \n",
    "        if in_ch != 3:\n",
    "            backbone[0] = nn.Conv2d(in_ch, 96, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        self.conv1 = nn.Sequential(*backbone[:3])\n",
    "        self.mp = backbone[3]\n",
    "        self.denseblock1 = backbone[4]\n",
    "        self.transition1 = backbone[5]\n",
    "        self.denseblock2 = backbone[6]\n",
    "        self.transition2 = backbone[7]\n",
    "        self.denseblock3 = backbone[8]\n",
    "        self.transition3 = backbone[9]\n",
    "        self.denseblock4 = backbone[10]\n",
    "        self.bn = backbone[11]\n",
    "        self.up1 = _Up(x1_ch=2208, x2_ch=2112, out_ch=768)\n",
    "        self.up2 = _Up(x1_ch=768, x2_ch=768, out_ch=384)\n",
    "        self.up3 = _Up(x1_ch=384, x2_ch=384, out_ch=96)\n",
    "        self.up4 = _Up(x1_ch=96, x2_ch=96, out_ch=96)\n",
    "        self.up5 = nn.Sequential(\n",
    "            _Interpolate(),\n",
    "            nn.BatchNorm2d(num_features=96),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=96, out_channels=64, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=out_ch, kernel_size=1)\n",
    "        \n",
    "        self.up1_conv = nn.Conv2d(in_channels=768, out_channels=out_ch, kernel_size=1)\n",
    "        self.up2_conv = nn.Conv2d(in_channels=384, out_channels=out_ch, kernel_size=1)\n",
    "        self.up3_conv = nn.Conv2d(in_channels=96, out_channels=out_ch, kernel_size=1)\n",
    "        self.up4_conv = nn.Conv2d(in_channels=96, out_channels=out_ch, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x_ = self.mp(x)\n",
    "        x1 = self.denseblock1(x_)\n",
    "        x1t = self.transition1(x1)\n",
    "        x2 = self.denseblock2(x1t)\n",
    "        x2t = self.transition2(x2)\n",
    "        x3 = self.denseblock3(x2t)\n",
    "        x3t = self.transition3(x3)\n",
    "        x4 = self.denseblock4(x3t)\n",
    "        x4 = self.bn(x4)\n",
    "        x5 = self.up1(x4, x3)\n",
    "        x6 = self.up2(x5, x2)\n",
    "        x7 = self.up3(x6, x1)\n",
    "        x8 = self.up4(x7, x)\n",
    "        feat = self.up5(x8)\n",
    "        cls = self.conv2(feat)\n",
    "        \n",
    "        up1_cls = self.up1_conv(x5)\n",
    "        up2_cls = self.up2_conv(x6)\n",
    "        up3_cls = self.up3_conv(x7)\n",
    "        up4_cls = self.up4_conv(x8)\n",
    "        \n",
    "        return {'output': cls, 'up1_cls': up1_cls, 'up2_cls': up2_cls, 'up3_cls': up3_cls, 'up4_cls': up4_cls}\n",
    "\n",
    "\n",
    "class _Interpolate(nn.Module):\n",
    "    def __init__(self, scale_factor=2, mode='bilinear', align_corners=True):\n",
    "        super(_Interpolate, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "        self.align_corners = align_corners\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.interpolate(x, scale_factor=self.scale_factor, mode=self.mode,\n",
    "                                      align_corners=self.align_corners)\n",
    "        return x\n",
    "\n",
    "\n",
    "class _Up(nn.Module):\n",
    "    def __init__(self, x1_ch, x2_ch, out_ch):\n",
    "        super(_Up, self).__init__()\n",
    "        self.up = _Interpolate()\n",
    "        self.conv1x1 = nn.Conv2d(in_channels=x2_ch, out_channels=x1_ch, kernel_size=1)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=x1_ch, out_channels=out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x2 = self.conv1x1(x2)\n",
    "        x = x1 + x2\n",
    "        x = self.conv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 模型加载\n",
    "import torch\n",
    "from pathlib2 import Path\n",
    "\n",
    "def save(epoch, net, optimizer, root):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'net': net.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, root)\n",
    "\n",
    "def _key_exist(data, cp, key):\n",
    "    return key in data and data[key] and key in cp and cp[key]\n",
    "\n",
    "def load_params(data, cp_file, device='cpu'):\n",
    "\n",
    "    cp_file = Path(cp_file)\n",
    "    assert cp_file.exists()\n",
    "\n",
    "    cp = torch.load(str(cp_file), map_location=device)\n",
    "    if _key_exist(data, cp, key='net'):\n",
    "        data['net'].load_state_dict(cp['net'])\n",
    "\n",
    "    if _key_exist(data, cp, key='optimizer'):\n",
    "        data['optimizer'].load_state_dict(cp['optimizer'])\n",
    "        for state in data['optimizer'].state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "    if _key_exist(data, cp, key='epoch'):\n",
    "        data['epoch'] = cp['epoch']\n",
    "\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编写可视化\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def numpy_to_plt(img):\n",
    "    return img.transpose((1, 2, 0))\n",
    "\n",
    "def imshow(title, imgs, shape=None, subtitle=None, cmap=None, transpose=False, pause=0.001, pltshow=True):\n",
    "    if type(imgs) is tuple:\n",
    "        num = len(imgs)\n",
    "        if shape is not None:\n",
    "            assert shape[0] * shape[1] == num\n",
    "        else:\n",
    "            shape = (1, num)\n",
    "        \n",
    "        if type(subtitle) is not tuple:\n",
    "            subtitle = (subtitle,) * num\n",
    "        else:\n",
    "            assert len(subtitle) == num\n",
    "        \n",
    "        if type(cmap) is not tuple:\n",
    "            cmap = (cmap,) * num\n",
    "        else:\n",
    "            assert len(cmap) == num\n",
    "        \n",
    "        fig = plt.figure(num=title, figsize=(shape[1] * 3, shape[0] * 3 + 0.5))\n",
    "        fig.clf()\n",
    "        fig.suptitle(title)\n",
    "        \n",
    "        fig.subplots(shape[0], shape[1], sharex=True, sharey=True)\n",
    "        axes = fig.get_axes()\n",
    "        \n",
    "        for i in range(shape[0]):\n",
    "            for j in range(shape[1]):\n",
    "                idx = i * shape[1] + j\n",
    "                axes[idx].set_title(subtitle[idx])\n",
    "                \n",
    "                cm = cmap[idx]\n",
    "                img = imgs[idx]\n",
    "                if cmap[idx] is None and len(img.shape) == 3:\n",
    "                    if img.shape[0] == 1 or len(img.shape) == 2:\n",
    "                        cm = 'gray'\n",
    "                        if len(img.shape) == 3 and img.shape[0] == 1:\n",
    "                            img = img.reshape((img.shape[1], img.shape[2]))\n",
    "                    elif img.shape[0] == 3:\n",
    "                        img = numpy_to_plt(img)\n",
    "                axes[idx].imshow(img, cm)\n",
    "    \n",
    "    else:\n",
    "        if transpose:\n",
    "            imgs = numpy_to_plt(imgs)\n",
    "        plt.figure(num=title)\n",
    "        plt.suptitle(title)\n",
    "        plt.title(subtitle)\n",
    "        plt.imshow(imgs, cmap)\n",
    "    \n",
    "    if pltshow:\n",
    "        plt.ion()\n",
    "        plt.show()\n",
    "        plt.pause(pause)\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理dataset\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from pathlib2 import Path\n",
    "from albumentations import Compose, PadIfNeeded, Resize\n",
    "class TestSet(data.Dataset):\n",
    "\n",
    "    def __init__(self, root, stack_num=1, img_size=(512, 512), transform=None):\n",
    "        self._root = Path(root)\n",
    "        self._stack_num = stack_num\n",
    "\n",
    "        self._img_size = img_size\n",
    "        self._transform = transform\n",
    "\n",
    "        self._get_data()\n",
    "\n",
    "        self._img_channels = self.__getitem__(0)['image'].shape[0]\n",
    "\n",
    "    def _get_data(self, extentions=['*.jpg', '*.png', '*.npy']):\n",
    "        self._imgs = []\n",
    "\n",
    "        for extention in extentions:\n",
    "            imgs = list(self._root.glob(extention))\n",
    "            self._imgs += imgs\n",
    "\n",
    "    def get_stack_num(self, idx):\n",
    "        data_path = self._imgs[idx]\n",
    "\n",
    "        if '.npy' in data_path.parts[-1]:\n",
    "            img = self.__npydata(data_path)\n",
    "            imgs = [img]*self._stack_num\n",
    "            imgs = np.stack(imgs, axis=2)\n",
    "            data = {'image': imgs, 'label': None}\n",
    "\n",
    "            return data\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.get_stack_num(idx)\n",
    "        data = self._transform(data)\n",
    "        data = self._default_transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._imgs)\n",
    "\n",
    "    def __npydata(self, data_path):\n",
    "        return np.load(str(data_path))\n",
    "\n",
    "    def __imgdata(self, data_path):\n",
    "        return cv2.cvtColor(cv2.imread(str(data_path)), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    def vis_transform(self, data):\n",
    "        cmap = [[0, 0, 0], [0, 255, 0], [0, 0, 255]]\n",
    "        cmap = np.array(cmap, dtype=np.int)\n",
    "        if 'image' in data.keys() and data['image'] is not None:\n",
    "            imgs = data['image']\n",
    "            if type(imgs).__module__ != np.__name__:\n",
    "                imgs = imgs.cpu().detach().numpy()\n",
    "            data['image'] = imgs\n",
    "        \n",
    "        if 'label' in data.keys() and data['label'] is not None and data['label'].shape[-1] != 0:\n",
    "            labels = data['label']\n",
    "            if type(labels).__module__ != np.__name__:\n",
    "                labels = labels.cpu().detach().numpy()\n",
    "            labels = cmap[labels]\n",
    "            labels = labels.transpose((0, 3, 1, 2))\n",
    "            labels = labels / 255\n",
    "            data['label'] = labels\n",
    "        \n",
    "        if 'predict' in data.keys() and data['predict'] is not None:\n",
    "            preds = data['predict']\n",
    "            if type(preds).__module__ != np.__name__:\n",
    "                preds = preds.cpu().detach().numpy()\n",
    "            # print(preds.shape)\n",
    "            if preds.shape[1] == self.num_classes:\n",
    "                preds = preds.argmax(axis=1)\n",
    "            preds = cmap[preds]\n",
    "            preds = preds.transpose((0, 3, 1, 2))\n",
    "            preds = preds / 255\n",
    "            data['predict'] = preds\n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "    def _resize(self, data):\n",
    "        data = to_numpy(data)\n",
    "        img, label = data['image'], data['label']\n",
    "        num = max(img.shape[0], img.shape[1])\n",
    "\n",
    "        aug = Compose_albu([\n",
    "            PadIfNeeded(min_height=num, min_width=num, border_mode=cv2.BORDER_CONSTANT, p=1),\n",
    "            Resize(height=self._img_size[0], width=self._img_size[1], p=1)\n",
    "        ])\n",
    "\n",
    "        data = aug(img=img, mask=label)\n",
    "        img, label = data['image'], data['mask']\n",
    "\n",
    "        data['image'] = img\n",
    "        data['label'] = label\n",
    "        return data\n",
    "\n",
    "    def _default_transform(self, data):\n",
    "        if (data['image'].shape[0], data['image'].shape[1]) != self._img_size:\n",
    "            data = self._resize(data)\n",
    "        \n",
    "        image, label = data['image'], data['label']\n",
    "        \n",
    "        image = image.astype(np.float32)\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        image = torch.from_numpy(image)\n",
    "        data['image'] = image\n",
    "        data['label'] = torch.Tensor()\n",
    "        \n",
    "        return data\n",
    "\n",
    "    @property\n",
    "    def img_channels(self):\n",
    "        return self._img_channels\n",
    "\n",
    "    @property\n",
    "    def spec_classes(self):\n",
    "        return 3\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 编写测试,包括用户上传单图和多图\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib2 import Path\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "import time\n",
    "\n",
    "\n",
    "def main(batch_size, img_size, data_path, resume, output_path):\n",
    "    data_path = Path(data_path)\n",
    "    output_path = Path(output_path)\n",
    "    assert data_path.exists()\n",
    "    if not output_path.exists():\n",
    "        output_path.mkdir(parents=True)\n",
    "    \n",
    "    transform = MedicalTransform(output_size=img_size)\n",
    "    dataset = TestSet(root=data_path, stack_num=3, img_size=img_size, transform=transform)\n",
    "    print(dataset.img_channels)\n",
    "    net = DenseUNet(in_ch=dataset.spec_classes, out_ch=dataset.img_channels)\n",
    "    #加载模型\n",
    "    cp_file = Path(resume)\n",
    "    assert cp_file.exists()\n",
    "\n",
    "    data = {'net': net}\n",
    "    load_params(data, cp_file)\n",
    "    print(f'{\" Start evaluation \":-^40s}\\n')\n",
    "    msg = f'Net: {net.__class__.__name__}\\n' + \\\n",
    "          f'Batch size: {batch_size}\\n'\n",
    "    print(msg)\n",
    "\n",
    "    net.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    sampler = SequentialSampler(dataset)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=1, pin_memory=True)\n",
    "\n",
    "    for batch_idx, data in enumerate(data_loader):\n",
    "        imgs = data['image'].cpu()\n",
    "        s = time.time()\n",
    "        outputs = net(imgs)\n",
    "        print(\"total time:\", time.time()-s)\n",
    "        predicts = outputs['output']\n",
    "        predicts = predicts.argmax(dim=1)\n",
    "        predicts = predicts.cpu().detach().numpy()\n",
    "        # print(np.unique(predicts))\n",
    "        data['predict'] = predicts\n",
    "        print(data['image'].shape, predicts.shape)\n",
    "        data = dataset.vis_transform(data)\n",
    "        \n",
    "        imgs, predicts = data['image'], data['predict']\n",
    "        # print(imgs.shape, predicts.shape)\n",
    "        # imshow(title='test', imgs=(imgs[0, 1], predicts[0]),shape=(1, 2), subtitle=('image', 'predict'))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3\n----------- Start evaluation -----------\n\nNet: DenseUNet\nBatch size: 1\n\ntotal time: 4.249403715133667\ntorch.Size([1, 3, 320, 320]) (1, 320, 320)\ntotal time: 2.7039954662323\ntorch.Size([1, 3, 320, 320]) (1, 320, 320)\ntotal time: 2.865765333175659\ntorch.Size([1, 3, 320, 320]) (1, 320, 320)\ntotal time: 2.821321487426758\ntorch.Size([1, 3, 320, 320]) (1, 320, 320)\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3da396b3f063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m320\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/0/imaging'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoint/best.pth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-861fac6e63f2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(batch_size, img_size, data_path, resume, output_path)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"total time:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mpredicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/unet-A7D43xXk/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-f5d07d0618e1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdenseblock1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mx1t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdenseblock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/unet-A7D43xXk/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/unet-A7D43xXk/lib/python3.7/site-packages/torchvision/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, init_features)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minit_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/unet-A7D43xXk/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/unet-A7D43xXk/lib/python3.7/site-packages/torchvision/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mbottleneck_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottleneck_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_rate\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             new_features = F.dropout(new_features, p=self.drop_rate,\n",
      "\u001b[0;32m~/.local/share/virtualenvs/unet-A7D43xXk/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/unet-A7D43xXk/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/unet-A7D43xXk/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    344\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    345\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 346\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(1, (320, 320), 'data/0/imaging', 'checkpoint/best.pth','out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bitunetpipenvca91bc3e4db7453ba98fe39018257c8c",
   "display_name": "Python 3.7.3 64-bit ('unet': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}